{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Whereabout Streets Data Extraction\n",
    "This notebook will demonstrate how to access Street and Bridge Operations PDF file and extract this data to create a work order plan template.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://upload.wikimedia.org/wikipedia/en/9/94/Closeup_of_pavement_with_grass.JPG\" /></div>\n",
    "\n",
    "## Introduction\n",
    "The purpose of this notebook is to create a Street and Bridge Work Order plans based on segment IDs and additional comments on long line. Markings feature layers are published in the City of Austin ArcGIS Portal page available for public view as well. \n",
    "\n",
    "The schedule for where sealcoat and overlay streets are completed is received through email by Street and Bridge Operations on a daily basis. It is sent as a PDF file that lists weather conditions, temperature, and provides a table of streets where paving is completed.\n",
    "\n",
    "<b>The only manual process the user will have to do is to:</b>\n",
    "- Input Segment IDs\n",
    "- Make comments on long line markings\n",
    "- Specify file path to retrieve the table of completed streets paved for PDF name and file path\n",
    "- Create any missing markings assets that are not visible in aerial imagery\n",
    "\n",
    "This process will cut down on the previous process of manually editing a plans layout through copy-pasting imagery and writing Location IDs, work groups, markings found, and the exporting plans one at a time. An excel document will be created based on this input and read segment IDs to find all short line and specialty point markings. This will ideally generate multiple PDF plans in a faster and shorter time frame.\n",
    "\n",
    "<i><b>Disclaimer:</b> This product is for informational purposes and may not have been prepared for or be suitable for legal, engineering, or surveying purposes. No warranty is made by the City of Austin regarding specific accuracy or completeness.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "The packages used for this project are:\n",
    "- [pandas](https://pandas.pydata.org/) to create dataframe of extracted table and transform the data\n",
    "- [openpyxl](https://openpyxl.readthedocs.io/en/stable/) to edit excel files\n",
    "- [arcgis](https://esri.github.io/arcgis-python-api/apidoc/html/) to search for markings feature layer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%run C:\\Users\\Govs\\Projects/CopyGISFeatures.py\n",
    "\n",
    "from openpyxl import Workbook,load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES !!!!!!!!!\n",
    "- Might remove arcgis.gis and move markings asset calculations to second notebook to use GISMAINT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "The date by month and day constant will determine the file pdf name to use as a dataframe. Folder path will determine where the plans will be created depending on the year. This is set to the top for the purpose of changing these constants as needed.\n",
    "\n",
    "<i>The table below explains the purpose of each constant.</i>\n",
    "\n",
    "| Constant | Description   |\n",
    "|:--------:|----|\n",
    "| <b>MONTH, DAY, YEAR</b> |Date used to find PDF in month-day format and file path based on year|\n",
    "|<b>FOLDER</b>      |File directory used to import SBO whereabouts reports from email|\n",
    "|<b>FILE_NAME</b>   |File directory name used to extact SBO whereabouts reports from file|\n",
    "|<b>SIGN_IN</b>   |Whether to prompt user to sign in to outlook email|\n",
    "|<b>INPUT</b>|Whether to prompt user to input segment Ids and comments to export to excel| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'FOLDER' (str)\n",
      "Stored 'EXCEL_FILE' (str)\n"
     ]
    }
   ],
   "source": [
    "YEAR = str(2019)\n",
    "FOLDER = (r\"G:\\ATD\\Signs_and_Markings\\MARKINGS\\Whereabouts WORK ORDERS\\{}\\Whereabouts_Summary\").format(YEAR)\n",
    "FILE_NAME = FOLDER + r'\\SBO_Combined'\n",
    "EXCEL_FILE = FILE_NAME + \".xlsx\"\n",
    "INPUT= True\n",
    "%store FOLDER\n",
    "%store EXCEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {'altref':'Location ID','on_street':'Street','from_street':'From','to_street':'To',\n",
    "        'crew':'Type','actfinish_1':'Finish Date'}\n",
    "values = {'OVL1':'Overlay','SLCT1':'Sealcoat','SLCT2':'Sealcoat','MILL':'Mill','DISTN1':'Overlay (DIS)'}\n",
    "\n",
    "sealcoat = pd.read_csv(FOLDER + r'\\sbo_seal_coat_2019.csv')\n",
    "overlay = pd.read_csv(FOLDER + r'\\sbo_overlay_2019.csv')\n",
    "df = sealcoat.append(overlay,sort=True).replace(values).filter(items=list(cols.keys())).sort_values('altref').rename(\n",
    "    columns=cols).reset_index(drop=True)\n",
    "df['Location'] = (df[\"Street\"] + ' FROM ' + df[\"From\"] + ' TO ' + df[\"To\"]).str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display the first 10 rows of the report from SBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location ID</th>\n",
       "      <th>Street</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Type</th>\n",
       "      <th>Finish Date</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30887.0</td>\n",
       "      <td>GRANDVIEW ST</td>\n",
       "      <td>31St St W</td>\n",
       "      <td>34Th St W</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:07 PM</td>\n",
       "      <td>GRANDVIEW ST FROM 31ST ST W TO 34TH ST W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34534.0</td>\n",
       "      <td>GLISSMAN RD</td>\n",
       "      <td>Springdale Rd</td>\n",
       "      <td>Mansell Ave</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:11 PM</td>\n",
       "      <td>GLISSMAN RD FROM SPRINGDALE RD TO MANSELL AVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37936.0</td>\n",
       "      <td>25TH 1/2 ST W</td>\n",
       "      <td>San Gabriel St</td>\n",
       "      <td>Leon St</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:15 PM</td>\n",
       "      <td>25TH 1/2 ST W FROM SAN GABRIEL ST TO LEON ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37977.0</td>\n",
       "      <td>MLK BLVD W</td>\n",
       "      <td>RIO GRANDE</td>\n",
       "      <td>PEARL ST</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:19 PM</td>\n",
       "      <td>MLK BLVD W FROM RIO GRANDE TO PEARL ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37986.0</td>\n",
       "      <td>SHOAL CREST AVE</td>\n",
       "      <td>28Th 1/2 St W</td>\n",
       "      <td>29Th St W</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:09 PM</td>\n",
       "      <td>SHOAL CREST AVE FROM 28TH 1/2 ST W TO 29TH ST W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39099.0</td>\n",
       "      <td>3RD ST E</td>\n",
       "      <td>San Saba St</td>\n",
       "      <td>Tillery Sq</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 1:36 PM</td>\n",
       "      <td>3RD ST E FROM SAN SABA ST TO TILLERY SQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39168.0</td>\n",
       "      <td>FAIRVIEW DR</td>\n",
       "      <td>4500</td>\n",
       "      <td>BIG BEND DR</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 23, 2019, 10:38 AM</td>\n",
       "      <td>FAIRVIEW DR FROM 4500 TO BIG BEND DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39187.0</td>\n",
       "      <td>HARTFORD RD</td>\n",
       "      <td>ENFIELD RD</td>\n",
       "      <td>WINDSOR RD</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Mar 21, 2019, 3:00 PM</td>\n",
       "      <td>HARTFORD RD FROM ENFIELD RD TO WINDSOR RD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39234.0</td>\n",
       "      <td>MOUNTAIN LAUREL DR</td>\n",
       "      <td>EXPOSITION BLVD</td>\n",
       "      <td>2809</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 23, 2019, 10:36 AM</td>\n",
       "      <td>MOUNTAIN LAUREL DR FROM EXPOSITION BLVD TO 2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39265.0</td>\n",
       "      <td>ROBBINS PL</td>\n",
       "      <td>Vance Cir</td>\n",
       "      <td>22Nd St W</td>\n",
       "      <td>Overlay</td>\n",
       "      <td>Jul 19, 2019, 3:13 PM</td>\n",
       "      <td>ROBBINS PL FROM VANCE CIR TO 22ND ST W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location ID              Street             From           To     Type  \\\n",
       "0      30887.0        GRANDVIEW ST        31St St W    34Th St W  Overlay   \n",
       "1      34534.0         GLISSMAN RD    Springdale Rd  Mansell Ave  Overlay   \n",
       "2      37936.0       25TH 1/2 ST W   San Gabriel St      Leon St  Overlay   \n",
       "3      37977.0          MLK BLVD W       RIO GRANDE     PEARL ST  Overlay   \n",
       "4      37986.0     SHOAL CREST AVE    28Th 1/2 St W    29Th St W  Overlay   \n",
       "5      39099.0            3RD ST E      San Saba St   Tillery Sq  Overlay   \n",
       "6      39168.0         FAIRVIEW DR             4500  BIG BEND DR  Overlay   \n",
       "7      39187.0         HARTFORD RD       ENFIELD RD   WINDSOR RD  Overlay   \n",
       "8      39234.0  MOUNTAIN LAUREL DR  EXPOSITION BLVD         2809  Overlay   \n",
       "9      39265.0          ROBBINS PL        Vance Cir    22Nd St W  Overlay   \n",
       "\n",
       "              Finish Date                                         Location  \n",
       "0   Jul 19, 2019, 3:07 PM         GRANDVIEW ST FROM 31ST ST W TO 34TH ST W  \n",
       "1   Jul 19, 2019, 3:11 PM    GLISSMAN RD FROM SPRINGDALE RD TO MANSELL AVE  \n",
       "2   Jul 19, 2019, 3:15 PM     25TH 1/2 ST W FROM SAN GABRIEL ST TO LEON ST  \n",
       "3   Jul 19, 2019, 3:19 PM           MLK BLVD W FROM RIO GRANDE TO PEARL ST  \n",
       "4   Jul 19, 2019, 3:09 PM  SHOAL CREST AVE FROM 28TH 1/2 ST W TO 29TH ST W  \n",
       "5   Jul 19, 2019, 1:36 PM          3RD ST E FROM SAN SABA ST TO TILLERY SQ  \n",
       "6  Jul 23, 2019, 10:38 AM             FAIRVIEW DR FROM 4500 TO BIG BEND DR  \n",
       "7   Mar 21, 2019, 3:00 PM        HARTFORD RD FROM ENFIELD RD TO WINDSOR RD  \n",
       "8  Jul 23, 2019, 10:36 AM  MOUNTAIN LAUREL DR FROM EXPOSITION BLVD TO 2809  \n",
       "9   Jul 19, 2019, 3:13 PM           ROBBINS PL FROM VANCE CIR TO 22ND ST W  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(10))\n",
    "df.to_csv(FILE_NAME + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "These functions will be used to extract and transform the data into a feasible format.\n",
    "\n",
    "<i>The table below explains the purpose of each:</i>\n",
    "\n",
    "| Method | Description   |\n",
    "|:--------:|----|\n",
    "|<b>lists_to_df</b> |Converts extracted nested list into a dataframe|\n",
    "|<b>pdf_table_to_df</b> |Extracts table from PDF and then converts to dataframe|\n",
    "|<b>input_form</b> |Prompts user to input segment IDs and long line specifications|\n",
    "|<b>query_df</b>   |Query dataframe by segment IDs|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts user to input segment IDs and longline while changing the datafram to include user input\n",
    "def input_form(df):\n",
    "    segments = longline = []\n",
    "    for index,row in df.iterrows():\n",
    "        console = input(row['Location'] + \"\\nSegment ID list: \")\n",
    "        if '\\t' in console:\n",
    "            temp = GISFeatures(console).to_df()\n",
    "            segments.append(str(list(temp.SEGMENT_ID))[1:-1])\n",
    "        else:\n",
    "            segments.append(None)\n",
    "        comment = input(\"Longline: \")\n",
    "        longline.append(comment)\n",
    "    df['Segment IDs'], df['LongLine'] = (segments,longline)\n",
    "    print(\"\\nInput complete.\")\n",
    "    return df\n",
    "    \n",
    "# Returns query dataframe appended if markings exist in the listed segment IDs\n",
    "def query_df(fc,index,f,df,df1):\n",
    "    q = \"SEGMENT_ID IN({})\".format(df[\"Segment IDs\"][index])\n",
    "    if q != \"SEGMENT_ID IN(N/A)\":\n",
    "        c = fc.query(where=q,return_count_only=True) \n",
    "        if c != 0:\n",
    "            sdf = fc.query(where=q).sdf.filter(items=f)\n",
    "            sdf[\"Location ID\"] = df[\"Location ID\"][index]\n",
    "            sdf[\"LongLine\"] = df[\"LongLine\"][index]\n",
    "            df1 = df1.append(sdf,sort=True)\n",
    "    df1['COUNTS'] = 1\n",
    "    return df1\n",
    "\n",
    "# Rename markings sp based on domain code\n",
    "def specialty_markings(df,field):\n",
    "    if field in df.columns:\n",
    "        renameList = list(zip(list(df[field]),list(df.SPECIALTY_POINT_SUB_TYPE)))\n",
    "        word = [\"Stop\",\"Yield\",\"Ahead\",\"Only\",\"Merge\",\"Ped\", \"X-ing\",\"MPH\",\"Bus Only\",\"Ped X-ing\",\"Keep Clear\",\"Do Not Block\"]\n",
    "        arrow = [\"Through\",\"Left\",\"Right\",\"Left/Right\",\"Left/Right/Through\",\"Left/Through\",\"Right/Through\",\n",
    "                 \"U-turn\",\"Lane reduction\",\"Wrong way\",\"Bike\"]\n",
    "        other = [\"Green pad\", \"Green launch pad\", \"Speed hump marking\",\"Diagonal crosshatch\", \"Chevron crosshatch\"]\n",
    "        parking = [\"Parking 'L'\", \"Parking 'T'\", \"Parking stall line\", \"Handicap symbol\"]\n",
    "        symbol = [\"Bike\",\"Shared lane (Sharrow)\",\"Bicyclist\",\"Railroad Crossing (RxR)\",\"Chevron\",\"Pedestrian\",\"Diamond\"]\n",
    "        rpm = ['blue','']\n",
    "        t =['word','arrow','symbol','','','rpm']\n",
    "        st = [word,arrow,symbol,other,parking,rpm]\n",
    "        index = 0\n",
    "        for i in renameList:\n",
    "            x = list(map(int,list(i)))\n",
    "            temp = st[x[0] - 1][x[1] - 1] + \" \" + t[x[0] - 1]\n",
    "            renameList[index] = temp\n",
    "            index += 1\n",
    "        df[field] = renameList\n",
    "        return df.drop('SPECIALTY_POINT_SUB_TYPE',axis=1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Return dataframe of the listed specifications\n",
    "def specifications(df,i):\n",
    "    df[\"SPECIFICATIONS\"] = ''\n",
    "    for index,row in df.iterrows():\n",
    "        keys = list(row[i:])\n",
    "        values = list(df.columns)[i:]\n",
    "        spec = []\n",
    "        for k,v in zip(keys,values):\n",
    "            if k != 'N/A' and k != '' and v != 'WORK GROUPS':\n",
    "                spec.append('{} {}'.format(int(k),v.lower().replace('_',' ')))\n",
    "            if row['LongLine'] != 'N/A':\n",
    "                sentence = 'Install {}, '.format(row['LongLine']) + ', '.join(word for word in spec)\n",
    "            else:\n",
    "                sentence = 'Install ' + ', '.join(word for word in spec)\n",
    "        df.at[index,'SPECIFICATIONS'] = sentence\n",
    "    if 'WORK GROUPS' in df.columns:\n",
    "        df.loc[df.Street != None,'WORK GROUPS'] = df.loc[df.Street != None,'WORK GROUPS'].apply(str)\n",
    "    return df\n",
    "\n",
    "# Returns dataframe of markings count and pages\n",
    "def location_in_df(df,markings_type,workgroup):\n",
    "    if 'Location ID' in df:\n",
    "        count = df.groupby(['Location ID',markings_type]).count()[['SEGMENT_ID']].rename(columns={\"SEGMENT_ID\":'COUNTS'})\n",
    "        count = count.pivot_table(values='COUNTS',index='Location ID',columns=(markings_type),aggfunc='first').reset_index()\n",
    "        count[workgroup] = workgroup\n",
    "        page = df.groupby(['Location ID','SEGMENT_ID','LongLine',markings_type]).count()[['COUNTS']]\n",
    "        page = page.pivot_table(\n",
    "            values='COUNTS',index=['Location ID','SEGMENT_ID','LongLine'],columns=(markings_type),aggfunc='first')\n",
    "        return count,page\n",
    "\n",
    "# Returns dataframe of cover page\n",
    "def create_cover(cover,sl_count,sp_count,wg):\n",
    "    cover.loc[cover.LongLine != 'N/A', wg[2]] = wg[2]\n",
    "    cover.loc[cover.LongLine == 'N/A', wg[2]] = 'N/A' \n",
    "    if not sl_count.empty and not sp_count.empty:\n",
    "        cover = reduce(lambda z,y: pd.merge_ordered(z,y,on='Location ID'), [cover,sl_count,sp_count])\n",
    "    elif not sl_count.empty or not sp_count.empty:\n",
    "        count = sl_count if sp_count.empty else sp_count\n",
    "        wg_remove = 'SPECIALTY MARKINGS' if sp_count.empty else 'SHORTLINE'\n",
    "        cover = pd.merge_ordered(count,sl_count,on='Location ID')\n",
    "        wg.remove(wg_remove)\n",
    "    else:\n",
    "        cover = specifications(cover,9)\n",
    "        return cover\n",
    "    cover = cover.dropna(how='all',subset=list(cover.columns)[6:]).fillna('N/A')\n",
    "    cover['WORK GROUPS'] = cover[wg].apply(','.join,1).apply(lambda x: [s for s in x.split(',') if s != 'N/A'])\n",
    "    cover = cover.drop(columns = wg).fillna('N/A')\n",
    "    cover = specifications(cover,9)\n",
    "    cover['PAGE'] = 1\n",
    "    return cover\n",
    "\n",
    "# Returns dataframe of pages\n",
    "def create_pages(pages,sl_page,sp_page):\n",
    "    if not sl_page.empty and not sp_page.empty:\n",
    "        pages = pd.merge_ordered(sl_page,sp_page,on=('Location ID','SEGMENT_ID','LongLine')).fillna(\"N/A\")\n",
    "        pages = specifications(pages,3)\n",
    "        pages = pd.merge_ordered(pages,streets,on=('Location ID','SEGMENT_ID','LongLine')).drop(columns='BLOCK')\n",
    "        pages = pages.sort_values(by=['Location ID','PAGE']).reset_index(drop = True)\n",
    "    elif not sl_page.empty or not sp_page.empty:\n",
    "        page = sl_page if sp_page.empty else sp_page\n",
    "        pages = specifications(page.fillna('N/A'),3)\n",
    "        pages = pd.merge_ordered(pages,streets,on=('Location ID','SEGMENT_ID','LongLine')).sort_values(\n",
    "            by=['BLOCK','Location ID']).reset_index(drop = True).drop(columns='BLOCK')\n",
    "        pages = pages.dropna(subset=['SPECIFICATIONS'])\n",
    "        page = 1\n",
    "        for index, row in streets.iterrows():\n",
    "            if index != 0 and (row['Location ID'] != pages['Location ID'][index - 1]):\n",
    "                page = 2\n",
    "                pages.at[index,'PAGE'] = page\n",
    "            else:\n",
    "                page += 1\n",
    "                pages.at[index,'PAGE'] = page\n",
    "    else:\n",
    "        pages.loc[cover.Street != None,'PAGE'] = 2\n",
    "    return pages\n",
    "\n",
    "# Creates worksheet in excel file unless the worksheet already exists\n",
    "def create_ws(df,sheet_name):\n",
    "    if sheet_name in wb:\n",
    "        del wb[sheet_name]\n",
    "    ws = wb.create_sheet(sheet_name)\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws.append(r)\n",
    "    wb.save(EXCEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "toc-hr-collapsed": false
   },
   "source": [
    "## Loading and Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF tables to Excel\n",
    "\n",
    "Now that the PDFs have been extracted and exported to the folder path, the next step is to extract the tables in the PDF and export it as an excel file.\n",
    "\n",
    "An input form will generate so the user can input Segment ID and comment information for each of the streets listed. The columns list will only take the relevant columns from the extracted table. The `pdfplumber` package will be used to extract tables from the PDF and prompt user to submit data.\n",
    "\n",
    "The input will be stored as a DataFrame saved to an excel document. If the user already provided input froma  previous session, the dataframe will be set to the excel file document instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = locations = longline = []\n",
    "with open(r'C:\\Users\\Govs\\Projects\\Files\\text console.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        if l.isupper():\n",
    "            sentence = l.replace('\\n','')\n",
    "            locations.append(sentence)\n",
    "        elif 'Segment ID list: ' in l:\n",
    "            if 'OBJECTID *' in l:\n",
    "                temp = GISFeatures(l).to_df()\n",
    "                segments.append(str(list(temp.SEGMENT_ID))[1:-1])\n",
    "            else:\n",
    "                segments.append(None)\n",
    "        else:\n",
    "            sentence = l[10:].replace('\\n','')\n",
    "            if sentence == '':\n",
    "                sentence = 'N/A'\n",
    "            longline.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Columns of extracted table\n",
    "columns = [\"Location ID\", \"Street\", \"From\", \"To\"]\n",
    "\n",
    "# Will prompt input and export to excel unless the excel file already exists. In that case it will read excel file instead\n",
    "if Path(FILE_NAME + '.csv').exists():\n",
    "    df = pd.read_csv(FILE_NAME + '.csv',index_col=0)\n",
    "    df = df.fillna(\"N/A\")\n",
    "else:\n",
    "    input_form(df)\n",
    "    df = df.fillna(\"N/A\")\n",
    "    df.to_excel(EXCEL_FILE)\n",
    "    df.to_csv(FILE_NAME + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1['Location'] = locations\n",
    "df1['Segment IDs'] = segments\n",
    "df1['LongLine'] = longline\n",
    "df2 = df.merge(df1,on='Location')\n",
    "df2 = df2[df2[\"Segment IDs\"] == df2[\"Segment IDs\"]]\n",
    "df2.to_excel(EXCEL_FILE)\n",
    "df2.to_csv(FILE_NAME + '.csv')\n",
    "df = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"N/A\")\n",
    "df['LongLine'] = ['N/A' if x == ' ' else x for x in df['LongLine']]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains a table for the list of streets with the following columns:\n",
    "- <i>Location ID</i>: unique identifier used for street paving\n",
    "- <i>Street</i>: main street that is paved\n",
    "- <i>From</i>: intersecting cross street\n",
    "- <i>To</i>: intersecting cross street\n",
    "- <i>Segment IDs</i>: list of segment IDs where street is paved seperated by commas\n",
    "- <i>Comments</i>: Notes on long line markings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Layer Data Query\n",
    "\n",
    "The next task is to find the markings through the list of segment IDs the user has inputted. For this task the `arcgis` package will be useful for extracting the markings available in each segment ID since the dataset is already available publically.\n",
    "\n",
    "Since the markings datasets are publically available, we can login to ArcGIS Online anonymously. \n",
    "\n",
    "Use `client_id` instead of `None` if you wish to log-in through an AGOL federate account. Note that it will prompt user to enter code which can be found by following the instructions. Going through an AGOL federated account is useful if the user wishes to add their own layers as a reference such as [NearMap](https://go.nearmap.com/) aerial imagery. \n",
    "\n",
    "It will search through the markings feature layer based on the list of segment IDs provided by the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# variables used to find and query feature layer in AGOL\n",
    "gis = GIS(\"https://austin.maps.arcgis.com/home/index.html\")\n",
    "url = r\"https://services.arcgis.com/0L95CJ0VTaxqcmED/arcgis/rest/services/TRANSPORTATION_{}/FeatureServer/0\"\n",
    "sl,sp,streets = (pd.DataFrame(),pd.DataFrame(),pd.DataFrame())\n",
    "\n",
    "# Columns for data frame. Indexes: df (0), shortline (1-4), specialty point (3 to etc.)\n",
    "cols = ['SHORT_LINE_TYPE','SEGMENT_ID','SPECIALTY_POINT_TYPE','SPECIALTY_POINT_SUB_TYPE']\n",
    "s_col = ['LEFT_BLOCK_FROM','RIGHT_BLOCK_FROM','SEGMENT_ID']\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    streets = query_df(FeatureLayer(url.format(\"street_segment\")),index,s_col,df,streets)      \n",
    "    sl = query_df(FeatureLayer(url.format(\"markings_short_line\")),index,cols[:2],df,sl)\n",
    "    sp = query_df(FeatureLayer(url.format(\"markings_specialty_point\")),index,cols[1:],df,sp)\n",
    "sp = specialty_markings(sp,cols[2])\n",
    "\n",
    "# Order table\n",
    "streets['BLOCK'] = np.maximum(streets[s_col[0]],streets[s_col[1]])\n",
    "streets = streets.sort_values(by=['BLOCK','Location ID']).reset_index(drop = True)\n",
    "streets = streets.rename(columns={'COUNTS':'PAGE'}).drop(s_col[:2],axis=1)\n",
    "\n",
    "page = 1\n",
    "for index, row in streets.iterrows():\n",
    "    if index != 0 and (row['Location ID'] != streets['Location ID'][index - 1]):\n",
    "        page = 2\n",
    "        streets.at[index,'PAGE'] = page\n",
    "    else:\n",
    "        page += 1\n",
    "        streets.at[index,'PAGE'] = page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Plans Table Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cover Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg = ['SHORT LINE','SPECIALTY MARKINGS','LONGLINE']\n",
    "sl_count,sl_page = location_in_df(sl,'SHORT_LINE_TYPE',wg[0])\n",
    "sp_count,sp_page = location_in_df(sp,'SPECIALTY_POINT_TYPE',wg[1])\n",
    "cover = create_cover(df.copy(),sl_count,sp_count,wg)\n",
    "pages = create_pages(df.copy(),sl_page,sp_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe lists pavement markings queried by segment IDs with the following columns:\n",
    "- <i>LOCATION ID</i>: Unique identifier used for street paving\n",
    "- <i>COMMENTS</i>: Notes on long line markings\n",
    "- <i>WORK GROUPS</i>: Type of markings work group assigned to work order\n",
    "- <i>SPECIFICATIONS</i>: Lists all markings that need to be installed on work order.\n",
    "\n",
    "\n",
    "The dataframe will be saves in an excel sheet for it to be used again to generate the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cover)\n",
    "display(pages) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pages Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Worksheets of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = load_workbook(filename = EXCEL_FILE)\n",
    "create_ws(cover,'Cover')\n",
    "create_ws(pages,'Pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Whereabouts Plans\n",
    "To generate whereabout plans, we will have to use the `arcpy` package, which requires Python 2 and ArcMap 10.5. Eventually, this notebook will be able to use `arcpy` in Python 3.\n",
    "\n",
    "[Click here to access notebook](PlansTemplate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Create Spreadsheet of Completed Streets\n",
    "This is intended to report on extracted streets generated from the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Columns of extracted table\n",
    "columns = [\"Location ID\", \"Street\", \"From\", \"To\"]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    df.read_excel(FOLDER + \"\\\\SBO Street List.xlsx\")\n",
    "except:\n",
    "    for foldername,subfolders,files in os.walk(FOLDER):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                FILE_NAME = \"\\\\\".join((FOLDER,file[:-4]))\n",
    "                df1 = pdf_table_to_df(columns)\n",
    "                df1[\"filename\"] = file\n",
    "                df = df.append(df1,sort=True)\n",
    "    df.to_excel(FOLDER + \"\\\\SBO Street List.xlsx\",sheet_name=\"Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
